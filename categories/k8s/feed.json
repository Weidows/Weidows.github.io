{
    "version": "https://jsonfeed.org/version/1",
    "title": "⭐️齐下无贰⭐️ • All posts by \"k8s\" category",
    "description": "May all the beauty be bless.✨",
    "home_page_url": "https://blog.weidows.tech",
    "items": [
        {
            "id": "https://blog.weidows.tech/post/system/k8s/k8s/",
            "url": "https://blog.weidows.tech/post/system/k8s/k8s/",
            "title": "k8s",
            "date_published": "2024-08-07T14:29:27.000Z",
            "content_html": "<h1>k8s</h1>\n<!--\n * @?: *\n\n * @Author: Weidows\n * @LastEditors: Weidows\n * @LastEditTime: 2024-08-10 18:43:06\n * @FilePath: \\Blog-private\\source\\_posts\\system\\k8s\\k8s.md\n * @Description:\n * @!: *\n\n-->\n<blockquote class=\"pullquote mindmap mindmap-md\"><ul>\n<li><a href=\"#k8s\">k8s</a>\n<ul>\n<li><a href=\"#interview\">Interview</a>\n<ul>\n<li><a href=\"#distributed-scheduling\">Distributed-Scheduling</a>\n<ul>\n<li><a href=\"#background\">Background</a></li>\n<li><a href=\"#analyze\">Analyze</a></li>\n<li><a href=\"#arch-graph\">Arch-graph</a></li>\n<li><a href=\"#modules-design-doc\">Modules-Design-doc</a></li>\n<li><a href=\"#considerations\">Considerations</a></li>\n</ul>\n</li>\n<li><a href=\"#system-architecture-design\">System-Architecture-Design</a>\n<ul>\n<li><a href=\"#background-1\">Background</a></li>\n<li><a href=\"#arch-graph-1\">Arch-graph</a></li>\n<li><a href=\"#modules-design-docs\">Modules-Design-docs</a></li>\n</ul>\n</li>\n<li><a href=\"#gpu-optimization\">GPU-optimization</a>\n<ul>\n<li><a href=\"#background-2\">Background</a></li>\n<li><a href=\"#thought\">Thought</a>\n<ul>\n<li><a href=\"#cn\">CN</a></li>\n<li><a href=\"#en\">EN</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#%E5%80%9F%E7%89%A9%E8%A1%A8\">借物表</a></li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<p><a><img src= \"https://pan.weidows.tech/d/local/img/loading.svg\" data-lazy-src=\"https://pan.weidows.tech/d/local/img/divider.png\" alt=\"分割线\"></a></p>\n<h2 id=\"Interview\">Interview</h2>\n<blockquote>\n<p>From: <a href=\"https://github.com/cloudpilot-ai/interview\">https://github.com/cloudpilot-ai/interview</a></p>\n</blockquote>\n<h3 id=\"Distributed-Scheduling\">Distributed-Scheduling</h3>\n<h4 id=\"Background\">Background</h4>\n<p>In all cloud providers, like AWS, Google, and others, there are many spot instances. They are quite cheap (10% of the on-demand instances’ price), but after you buy them, they could be terminated with only two minutes’ notice in advance (in most scenarios, we don’t set PDB, and we should perform the graceful drain).</p>\n<p>So, I want you to design a strategy to maximize the use of spot instances without causing service interruptions, instead of relying solely on on-demand instances, to cut costs, by using distributed scheduling in a single cluster (on-demand/spot mixed or other methods for one workload). This is important because all spot instances being terminated at the same time could cause interruptions for different kinds of workloads (single replica workload, multiple replica workload).</p>\n<p>Also, I don’t want to change the scheduler already used in the K8s cluster and want to ensure the minimal components necessary in the cluster.</p>\n<p>Notes:</p>\n<blockquote>\n<ol>\n<li>On demand nodes has label: <a href=\"http://node.kubernetes.io/capacity:\">node.kubernetes.io/capacity:</a> on-demand.</li>\n<li>Spot node has label: <a href=\"http://node.kubernetes.io/capacity:\">node.kubernetes.io/capacity:</a> spot.</li>\n<li>Workloads represented as Deployments and StatefulSets.</li>\n<li>on-demand/spot instance represented as K8s nodes in the cluster.</li>\n<li>Only focus on scheduling control; the graceful drain after receiving the terminal notification is handled by other components.</li>\n</ol>\n</blockquote>\n<hr>\n<h4 id=\"Analyze\">Analyze</h4>\n<p>尽可能少的使用云平台 on-demand 实例, 转为使用 spot 降低成本</p>\n<p>spot drain 比较频繁, 需要通过设计一套 schedule system 去解决 graceful drain in 2min 的问题</p>\n<h4 id=\"Arch-graph\">Arch-graph</h4>\n<!-- 防止 mermaid 错乱 --><div class=\"mermaid-wrap\"><pre class=\"mermaid-src\" hidden>\n  graph TD\n    subgraph k8s cluster\n        A[On-demand node]\n        B[Spot node]\n    end\n\n    C[kube-scheduler]\n    D[Distributed Scheduling controler]\n    E[Event listener]\n    F[re-schedule]\n    G[k8s API Server]\n    H[Cloud Provider&#39;s notification]\n\n    A --&gt; C\n    B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    D --&gt; G\n    E --&gt; F\n    F --&gt; G\n    H --&gt; D\n  </pre></div>\n<h4 id=\"Modules-Design-doc\">Modules-Design-doc</h4>\n<ul>\n<li>\n<p>kube-scheduler:</p>\n<p>It handles the initial placement of Pods on the available nodes (either On-demand or Spot). It makes decisions based on resource availability, but it doesn’t handle the dynamic nature of Spot instances, which is why it passes control to the Distributed Scheduling Controller.</p>\n</li>\n<li>\n<p>Distributed Scheduling Controller:</p>\n<p>This is a custom controller that monitors the state of Spot nodes. It reacts to termination notifications from the cloud provider, triggering re-scheduling of Pods from Spot nodes to On-demand nodes when necessary. It interacts with both the <code>kube-scheduler</code> and <code>k8s API Server</code> to coordinate these actions.</p>\n</li>\n<li>\n<p>Event Listener:</p>\n<p>A sub-component of the Distributed Scheduling Controller, it listens for termination notifications from the Cloud Provider. When a notification is received, it initiates the re-scheduling process by triggering the necessary operations within the controller.</p>\n</li>\n<li>\n<p>re-schedule:</p>\n<p>This module represents the actual process of migrating Pods from Spot nodes to On-demand nodes. It ensures that services continue running smoothly by moving affected workloads to stable nodes before the Spot nodes are terminated.</p>\n</li>\n<li>\n<p>Cloud Provider’s Notification:</p>\n<p>This is the external signal from the cloud provider indicating that a Spot node is about to be terminated. The Distributed Scheduling Controller listens for this notification and triggers re-scheduling as needed.</p>\n</li>\n</ul>\n<hr>\n<h4 id=\"Considerations\">Considerations</h4>\n<ul>\n<li>\n<p>Handling Spot Instance Termination:</p>\n<p>The system must efficiently handle the rapid termination of Spot instances, ensuring that critical workloads are quickly migrated to On-demand nodes without causing service disruptions.</p>\n</li>\n<li>\n<p>Monitoring and Responsiveness:</p>\n<p>The Event Listener and Distributed Scheduling Controller must be highly responsive to termination notifications. Delays in re-scheduling could lead to service outages.</p>\n</li>\n<li>\n<p>Resource Management:</p>\n<p>It’s crucial to ensure that there are always enough resources available on On-demand nodes to accommodate Pods migrating from Spot nodes. Resource constraints can lead to scheduling failures.</p>\n</li>\n<li>\n<p>System Compatibility:</p>\n<p>The integration between the <code>kube-scheduler</code>, Distributed Scheduling Controller, and <code>k8s API Server</code> should be seamless to avoid conflicts in Pod scheduling and state management.</p>\n</li>\n<li>\n<p>Testing and Reliability:</p>\n<p>The system should be thoroughly tested to handle different failure scenarios, such as mass Spot instance terminations or API Server unavailability, to ensure it can recover gracefully.</p>\n</li>\n</ul>\n<p><a><img src= \"https://pan.weidows.tech/d/local/img/loading.svg\" data-lazy-src=\"https://pan.weidows.tech/d/local/img/divider.png\" alt=\"分割线\"></a></p>\n<h3 id=\"System-Architecture-Design\">System-Architecture-Design</h3>\n<h4 id=\"Background-2\">Background</h4>\n<p>Our rough system architecture is illustrated as follows:</p>\n<p><img src= \"https://pan.weidows.tech/d/local/img/loading.svg\" data-lazy-src=\"https://pan.weidows.tech/d/local/blog/%E5%89%AA%E8%B4%B4%E6%9D%BF-20240810.png\" alt=\"Architecture Diagram\"></p>\n<p>Customers install an agent component in their clusters, which pushes metrics to the API endpoint (<a href=\"https://api.xxx.com\">https://api.xxx.com</a>). On the server side, our API server processes and stores these metrics in the database. Another controller analyzes each cluster and provides optimization recommendations to customers, depicted as follows:</p>\n<p>I want you to outline a rough design document for the system that ensures high availability, security, and performance.</p>\n<blockquote>\n<p>Hint: This can be implemented using Kubernetes (K8s) technologies, such as HPA. But for different components, we need use different technologies.</p>\n</blockquote>\n<p>Notes:</p>\n<blockquote>\n<ol>\n<li>You don’t need to think about the database.</li>\n<li>For one customer’s cluster, only one agent is installed, it will scrap metrics from the customer’s cluster’s kube-apiserver.</li>\n<li>Focus on architecture design, not specific implementation.</li>\n</ol>\n</blockquote>\n<!--\n  下面我想提一个具有挑战性的问题, 有关k8s架构设计\n\n  背景是, 有服务端和客户端两侧 k8s cluster\n  客户端侧使用了很多云服务平台的 spot (比如 google / AWS 的), 所以 workload 会频繁变化\n  我想设计一套系统, 能够监测客户端侧的 k8s 运行环境, 将 node/workload(deployment, statefulset, namespace, replica) info 实时传到服务端侧\n\n  需求:\n  1. 我想在客户端侧部署一套环境监测程序, 在服务端搭建一套服务接收处理响应客户端的变化\n  2. 处理过程中肯定会用到k8s的各种组件, 请尽可能合理的采用主流和推荐的做法\n  3. 客户端侧的设计并不复杂, 但需要考虑到客户端会有成千上万个, 所以服务端侧需要满足高可用/高性能/安全, 另外服务端并不需要考虑Database的设计, 只需要将其抽象为一个DB模块就好\n\n  输出:\n  请用markdown的mermaid画一下架构图\n  按mermaid介绍一下每个模块/功能点用到了什么, 这么设计有什么优缺点\n\n\n  <a>![分割线](https://pan.weidows.tech/d/local/img/divider.png)</a>\n\n\n  有几个提问和需要考虑的地方\n  1. server side 在处理之后, 也需要向 client side 传输指令, 然后 client side 处理调度\n  2. 使用了 message queue, 与分布式调度框架比较的话, 有什么区别和优势, 如何后者更好的话可以换一下\n    将消息队列和分布式调度框架作比较时，有几个重要区别：\n      消息队列:\n        优势: 主要用于异步数据传输和解耦服务。消息队列有很强的持久化机制，可以确保消息的可靠传递。适合在需要处理大量并发数据的情况下，将数据流进行缓冲和排队，确保处理的稳定性和可扩展性。\n        局限: 需要额外的监控和管理。队列内的消息处理顺序以及延迟管理可能较为复杂。另外，在高复杂性任务调度上，消息队列可能略显不足。\n\n      分布式调度框架:\n        优势: 比如使用Apache Mesos、Kubernetes的`CronJob`、或Airflow，可以更有效地调度和执行复杂的任务。调度框架通常支持任务的依赖关系处理，能够自动化地对任务进行分配、监控和故障处理。\n        局限: 相比消息队列，调度框架在高并发的简单任务处理上可能效率较低，而且其实现和维护通常需要更高的复杂度。\n\n     结论: 如果系统主要是为了处理数据的高并发传输和处理，消息队列是合适的选择。如果涉及到复杂的任务调度和依赖关系管理，可以考虑引入分布式调度框架。你可以根据具体的需求，选择使用二者之一，或将二者结合使用。\n  3. Authentication 为什么要分开处理\n    将Authentication（认证）独立出来的原因主要有以下几点：\n\n    安全性: 独立的认证服务可以集中管理和更新身份验证策略，避免不同系统模块各自处理认证所带来的安全隐患。\n    复用性: 独立的认证服务可以支持多种客户端和接口的认证需求，方便扩展。\n    可扩展性: 认证服务的独立性允许它根据负载情况进行扩展，而不影响其他服务的稳定性。\n\n    不过，如果系统较为简单，认证负载不大，可以考虑将认证功能集成到API Gateway中，以减少系统复杂性。\n  4. 有什么其他的k8s技术可以考虑加入使用, 比如 HPA, ETCD\n\n  请调整一下 mermaid, 并回答一下\n-->\n<h4 id=\"Arch-graph-2\">Arch-graph</h4>\n<!-- 防止 mermaid 错乱 --><div class=\"mermaid-wrap\"><pre class=\"mermaid-src\" hidden>\n  graph TD\n    subgraph Client Side\n        B([Monitoring Agent])\n        C[Command Executor]\n        D((K8s Cluster))\n        E[Cloud Platform]\n    end\n\n    subgraph Server Side\n        F[API Gateway]\n        G[Authorization Service]\n        H[Message Queue]\n        I[Worker Nodes]\n        J([Processing Service])\n        K[(Database Module)]\n        L[Instruction Generation Module]\n    end\n\n    B --&gt;|Collects Data| D\n    B --&gt;|Sends Data| F\n    E --&gt;|Notice| B\n    F --&gt; G\n    F --&gt;|Sends Instruction| B\n    F --&gt; H\n    G --&gt; J\n    H --&gt; I\n    I --&gt; J\n    J --&gt; K\n    J --&gt; L\n    L --&gt; F\n    B --&gt; C\n    C --&gt; D\n  </pre></div>\n<!--\n  ### 模块及功能介绍\n  1. Environment Monitoring Agent（环境监测程序）\n\n    功能: 部署在每个客户端的 K8s 集群中，负责采集节点信息（Node Info）、工作负载信息（Workload Info，如 Deployment, StatefulSet, Namespace, Replica 等）。这些信息将被打包并通过安全通道发送到服务端。\n    实现: 可以使用 Kubernetes 的原生工具，如`kube-state-metrics`和`node-exporter`，结合`custom controllers`或者`DaemonSet`进行数据采集。数据可以通过 gRPC 或 HTTP API 发送到服务端。\n    优点: 利用 Kubernetes 的原生工具，易于部署和维护，并且具有高扩展性。\n    缺点: 监控数据量大时可能对网络带宽和集群性能造成一定压力。\n\n  2. API Gateway（服务端 API 网关）\n\n    功能: 接收所有客户端的请求，统一处理认证和流量控制，确保服务端的安全性和可扩展性。\n    实现: 可以使用 Nginx 或 Kong 作为 API 网关，并通过 SSL/TLS 进行加密，确保数据传输安全。\n    优点: 提供了统一的入口和控制点，支持负载均衡、限流等功能，确保系统的安全性和稳定性。\n    缺点: 如果网关本身出现故障，会导致整个系统无法正常工作，因此需要高可用部署。\n\n  3. Authentication & Authorization Service（认证与授权服务）\n\n    功能: 确保只有合法的客户端能够接入服务端，并基于角色和权限控制数据的访问。\n    实现: 可以使用 OAuth 2.0 或 OpenID Connect 协议，结合 JWT (JSON Web Token) 进行认证授权。\n    优点: 强大的安全性和灵活的权限控制。\n    缺点: 需要维护认证服务的可用性和密钥的安全性。\n\n  4. Message Queue（消息队列）\n\n    功能: 负责将客户端发送的数据放入消息队列，以解耦数据接收和处理过程，提高系统的处理能力和可扩展性。\n    实现: 可以使用 Kafka 或 RabbitMQ 等高性能消息队列系统。\n    优点: 高吞吐量，支持消息持久化和重试机制，保证数据不会丢失。\n    缺点: 需要额外的资源和复杂的运维管理。\n\n  5. Worker Nodes（工作节点）\n\n    功能: 消费消息队列中的数据，并将数据传递给处理服务进行进一步处理。\n    实现: 可以使用 Kubernetes 的`Deployment`或`StatefulSet`进行工作节点的部署，结合 HPA（Horizontal Pod Autoscaler）实现自动扩展。\n    优点: 动态扩展能力强，能有效处理高并发任务。\n    缺点: 在高峰期，可能需要大量的计算资源支持。\n\n  6. Processing Service（数据处理服务）\n\n    功能: 处理从客户端采集的数据，执行必要的数据清洗、聚合，并将结果存储到数据库模块中。\n    实现: 可以使用基于微服务的架构进行设计，使用流处理框架如 Apache Flink 或 Storm，结合容器化的部署方式。\n    优点: 灵活的扩展性和强大的数据处理能力，适应多样化的处理需求。\n    缺点: 复杂度较高，需要额外的监控和管理。\n\n  7. Database Module（数据库模块）\n    功能: 存储处理后的数据，提供高效的查询和分析能力。\n    实现: 可以使用 NoSQL 数据库如 MongoDB 或 Cassandra，或者分布式 SQL 数据库如 CockroachDB，来保证数据的高可用和高性能。\n    优点: 高可用性和扩展性，适应大规模数据的存储和查询需求。\n    缺点: 数据库的选择和调优需要经验，可能会增加系统复杂性。\n\n  ### 设计分析\n\n  - 优点:\n\n    1. 高可用性：通过多层次的设计，确保系统在任何一层出现问题时，不会导致整体系统的崩溃。\n    2. 高性能：使用消息队列和可扩展的工作节点设计，能够处理大量并发的客户端请求。\n    3. 安全性：通过 API 网关和认证服务，确保数据传输和访问的安全性。\n    4. 可扩展性：客户端数量可以动态增长，服务端可以通过扩展工作节点和数据库模块来应对增加的负载。\n\n  - 缺点:\n    1. 复杂度：系统设计较为复杂，涉及多个组件和服务的集成，增加了部署和运维的难度。\n    2. 成本：由于涉及到多种高性能组件，如消息队列、工作节点、数据库等，可能会增加系统的整体成本。\n    3. 运维挑战：多组件的运维和监控要求较高，需要专业的团队进行管理，特别是消息队列和数据库模块的稳定性保障。\n\n  这种设计能有效满足大规模分布式 K8s 集群监控和数据处理的需求，同时也为系统的扩展性和高可用性提供了强有力的支持。\n\n\n  结合上面 mermaid 架构图, 用英文再次简介的总结一下每个功能模块设计的功能和必要性, 有什么优点和考虑之处\n-->\n<h4 id=\"Modules-Design-docs\">Modules-Design-docs</h4>\n<ol>\n<li>\n<p>Monitoring Agent (Client Side)<br>\nFunction: The Monitoring Agent is responsible for collecting real-time data from the client-side Kubernetes cluster and receiving notifications from the cloud platform, such as spot instance termination messages. It then sends this data to the Server Side API Gateway.</p>\n<p>Necessity: This module is crucial for maintaining an up-to-date overview of the client-side environment, enabling the system to react dynamically to changes.</p>\n<p>Advantages: Centralizes data collection and processing of cloud notifications, making the client-side environment more responsive and easier to manage.</p>\n</li>\n<li>\n<p>Command Executor (Client Side)<br>\nFunction: Receives instructions from the Monitoring Agent and executes them on the Kubernetes cluster. Before executing, it assesses the real-time environment to ensure optimal decision-making.</p>\n<p>Necessity: Ensures that commands are executed with consideration of the current state of the client environment, improving operational efficiency.</p>\n<p>Advantages: Adds a layer of decision-making that enhances the flexibility and responsiveness of the system, reducing the risk of executing commands that could negatively impact the environment.</p>\n</li>\n<li>\n<p>Kubernetes Cluster (Client Side)<br>\nFunction: Represents the client-side Kubernetes infrastructure where workloads are deployed and managed.</p>\n<p>Necessity: The core environment where all client-side applications run, making it the focal point of monitoring and command execution.</p>\n<p>Advantages: Allows for scalable and automated management of containerized applications.</p>\n</li>\n<li>\n<p>Cloud Platform (Client Side)<br>\nFunction: Sends spot instance termination notices or other relevant notifications to the Monitoring Agent.</p>\n<p>Necessity: Critical for handling the dynamic and volatile nature of spot instances, which are commonly used to optimize cost.</p>\n<p>Advantages: Enhances the system’s ability to preemptively respond to changes in the cloud environment, improving uptime and reducing potential disruptions.</p>\n</li>\n<li>\n<p>API Gateway (Server Side)<br>\nFunction: Acts as the central entry point for all data sent from the client side. It handles the routing of data to the appropriate backend services, including sending instructions back to the client side.</p>\n<p>Necessity: Essential for managing communication between the client and server sides, ensuring secure, scalable, and organized data flow.</p>\n<p>Advantages: Provides a centralized control point for authentication, authorization, and traffic management, enhancing the system’s overall security and scalability.</p>\n</li>\n<li>\n<p>Authorization Service (Server Side)<br>\nFunction: Handles the authentication and authorization of incoming data and commands, ensuring that only verified and authorized actions are processed.</p>\n<p>Necessity: Critical for maintaining the security and integrity of the system by preventing unauthorized access and operations.</p>\n<p>Advantages: Adds a strong security layer to the system, protecting against potential breaches and unauthorized data manipulation.</p>\n</li>\n<li>\n<p>Message Queue (Server Side)<br>\nFunction: Decouples the ingestion of data from its processing, allowing for high-throughput and reliable handling of large volumes of data.</p>\n<p>Necessity: Ensures that data from multiple clients can be processed asynchronously, improving system resilience and scalability.</p>\n<p>Advantages: Enhances the system’s ability to handle bursts of data and maintain performance under heavy load, reducing the likelihood of bottlenecks.</p>\n</li>\n<li>\n<p>Worker Nodes (Server Side)<br>\nFunction: Consume messages from the queue and process the data or commands as required.</p>\n<p>Necessity: Essential for executing the bulk of the system’s processing tasks in a distributed and scalable manner.</p>\n<p>Advantages: Supports horizontal scaling, allowing the system to handle increasing workloads by simply adding more worker nodes.</p>\n</li>\n<li>\n<p>Processing Service (Server Side)<br>\nFunction: Core service responsible for processing the data received from the client side, making decisions, and interacting with other server-side components like the database and instruction generation module.</p>\n<p>Necessity: Central to the system’s ability to analyze client data and generate appropriate responses or commands.</p>\n<p>Advantages: Provides a flexible and extensible processing framework that can be tailored to specific application needs.</p>\n</li>\n<li>\n<p>Database Module (Server Side)<br>\nFunction: Stores processed data, including logs, metrics, and historical records, for analysis and reporting.</p>\n<p>Necessity: Ensures that all critical data is persisted for future reference, analysis, and compliance requirements.</p>\n<p>Advantages: Offers high availability and scalability, ensuring that the system can store and retrieve data efficiently as it grows.</p>\n</li>\n<li>\n<p>Instruction Generation Module (Server Side)<br>\nFunction: Generates commands and instructions based on the processed data, which are then sent back to the client side via the API Gateway.</p>\n<p>Necessity: Enables the system to actively manage and optimize client-side operations based on real-time data.</p>\n<p>Advantages: Allows for automated, data-driven decision-making, enhancing the system’s overall efficiency and responsiveness.</p>\n</li>\n</ol>\n<p><a><img src= \"https://pan.weidows.tech/d/local/img/loading.svg\" data-lazy-src=\"https://pan.weidows.tech/d/local/img/divider.png\" alt=\"分割线\"></a></p>\n<h3 id=\"GPU-optimization\">GPU-optimization</h3>\n<h4 id=\"Background-3\">Background</h4>\n<p>Please open your mind. In the AI world today, every company uses GPUs for training and inference. Is there anything we can do to reduce the cost of GPUs for training and inference?</p>\n<p>Alternatively, are there other areas in the public cloud(K8s) that could be optimized?</p>\n<h4 id=\"Thought\">Thought</h4>\n<h5 id=\"CN\">CN</h5>\n<p>因为我之前待过的公司和创业的公司都是主要涉及 AI, 这块还算比较熟悉, 痛点考虑点如下:</p>\n<ol>\n<li>\n<p>AI 项目 (Python 为主) 可迁移性比较差, 与操作系统算是半强绑定</p>\n<p>不同于 CPU, GPU 是个原子化的东西, 没法再拆分了, 而且 GPU 对 CPU 的性能依赖也是线性关联</p>\n<p>torch 这边还算比较成熟/版本切换自由, 但还是有很多库, 乃至 cuda 对操作系统有侵入和依赖性</p>\n<p>如何考虑要让客户端容器能扩缩, 需要额外适配下如何完善的装好依赖</p>\n</li>\n<li>\n<p>train / inference 的任务对性能的优化并不是很好</p>\n<p>毕竟大多都是研究团队发 paper 用的, 代码很多是一团糟, 适配多进程都很少见, 更不用说适配容器化了</p>\n<p>涉及到利用容器技术进行扩缩, 不太乐观, 这需要开发者手动去适配代码, 不然扩到 100 核只占用满 10 核也是很有可能的</p>\n<p>train 的任务非常非常非常怕中断, 哪怕是开发者自己也说不准不同电脑上进行训练到底需要多高的性能, 所以一般会开性能冗余很多的机子</p>\n<p>数据集 IO 这块的话压力一般来说不算太过分, 可以考虑用 k8s 技术进行读共享, 写需要额外操作</p>\n</li>\n<li>\n<p>GPU 共享平台最近有雏形了, 利用容器技术搞这个应该不错</p>\n<p>毕竟确实有一些不在乎钱的团队整晚开着机子不用</p>\n<p>而且 GPU 租用平台的受众还是蛮大的, 能用上容器的话, 比常规整租 GPU 的平台应该能便宜些</p>\n</li>\n<li>\n<p>如果考虑可插入性的, 那就只有在 pod 内跑多个容器了</p>\n<p>不过还是有预测问题, AI 领域的应用, 跑起来后显存占用起伏比较大且随机</p>\n<p>会很容易出现突发性性能抢占</p>\n</li>\n</ol>\n<h5 id=\"EN\">EN</h5>\n<p>Since the companies I’ve worked for and the startups I’ve been involved in have primarily focused on AI, I’m fairly familiar with this area. The pain points and considerations are as follows:</p>\n<ol>\n<li>\n<p><strong>AI Project (Mainly Python) Portability Is Relatively Poor, With a Semi-Strong Binding to the Operating System</strong></p>\n<p>Unlike CPUs, GPUs are atomic entities and cannot be further split. Moreover, the performance of GPUs is linearly dependent on the CPU’s capabilities.</p>\n<p>While Torch is relatively mature and allows for flexible version switching, there are still many libraries, and even CUDA, that have invasive dependencies on the operating system. To ensure that a client container can scale up or down, additional adjustments may be necessary to ensure that dependencies are properly installed.</p>\n</li>\n<li>\n<p><strong>The Performance Optimization for Train/Inference Tasks Is Not Very Good</strong></p>\n<p>Since most of these tasks are designed by research teams for publishing papers, the code is often quite messy, with little adaptation for multiprocessing, let alone containerization.</p>\n<p>When it comes to utilizing container technology for scaling, the outlook is not very optimistic. Developers often need to manually adapt the code;<br>\notherwise, it’s possible to scale up to 100 cores but only fully utilize 10 of them.</p>\n<p>Training tasks are highly sensitive to interruptions, and even developers themselves cannot accurately predict the required performance on different machines for training, so they generally opt for machines with a significant performance surplus.</p>\n<p>The pressure on dataset I/O is generally not too excessive and can be managed with Kubernetes technology for read-sharing, but additional operations are needed for writes.</p>\n</li>\n<li>\n<p><strong>There Is a Preliminary GPU Sharing Platform, and Using Container Technology for This Seems Promising</strong></p>\n<p>After all, there are indeed teams that don’t mind leaving their machines running all night without use. Additionally, the market for GPU rental platforms is quite large.</p>\n<p>If container technology can be utilized, it could potentially be cheaper than conventional GPU rental platforms that require full-time GPU leasing.</p>\n</li>\n<li>\n<p><strong>If Considering Plug-and-Play Capability, Then Running Multiple Containers Within a Pod Is the Only Option</strong></p>\n<p>However, there is still the issue of prediction. In the field of AI applications, the memory usage can fluctuate significantly and randomly once the process starts running.</p>\n<p>This could easily lead to sudden performance contention.</p>\n</li>\n</ol>\n<p><a><img src= \"https://pan.weidows.tech/d/local/img/loading.svg\" data-lazy-src=\"https://pan.weidows.tech/d/local/img/divider.png\" alt=\"分割线\"></a></p>\n<h2 id=\"借物表\">借物表</h2>\n<p><a name='cite_note-1' href='#cite_ref-1'>[1]</a>: Google &amp; ChatGPT</p>\n<script type=\"text&#x2F;javascript\" src=\"https://unpkg.com/kity@2.0.4/dist/kity.min.js\"></script><script type=\"text&#x2F;javascript\" src=\"https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js\"></script><script defer=\"true\" type=\"text&#x2F;javascript\" src=\"https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js\"></script><link rel=\"stylesheet\" type=\"text&#x2F;css\" href=\"https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css\">",
            "tags": [
                "design",
                "system",
                "k8s"
            ]
        }
    ]
}