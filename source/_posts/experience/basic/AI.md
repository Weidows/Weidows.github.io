---
title: 🐊人工智能笔记
password: ""
tags:
  - 人工智能
  - 机器学习
katex: true
comments: true
aside: true
date: 2022-03-19 00:13:49
cover: https://www.helloimg.com/images/2022/03/19/RaxR4K.png
top_img:
---

# 人工智能笔记

<!--
 * @?: *********************************************************************
 * @Author: Weidows
 * @LastEditors: Weidows
 * @LastEditTime: 2022-03-19 02:15:34
 * @FilePath: \Blog-private\source\_posts\experience\basic\AI.md
 * @Description:
 * @!: *********************************************************************
-->

```pullquote mindmap mindmap-md
- [人工智能笔记](#人工智能笔记)
  - [体系概览](#体系概览)
  - [引例](#引例)
    - [little-sample](#little-sample)
    - [激活函数](#激活函数)
    - [感知器](#感知器)
      - [例题](#例题)
  - [借物表](#借物表)
```

- <details>

    <summary> 文章封面图 (恐怖慎看) </summary>

  ![RaxZ0b.png](https://www.helloimg.com/images/2022/03/19/RaxZ0b.png)

  ***

  </details>

- 人工智能是什么? 拿图说事
  <sup id='cite_ref-1'>[\[1\]](#cite_note-1)</sup>
  <sup id='cite_ref-2'>[\[2\]](#cite_note-2)</sup>

  > ![](https://www.helloimg.com/images/2022/02/27/GVidNr.png)\
  > ![](https://www.helloimg.com/images/2022/03/09/RCPHaM.png)

<a>![分割线](https://cdn.jsdelivr.net/gh/Weidows/Images/img/divider.png)</a>

## 体系概览

<!--  -->

```pullquote mindmap mindmap-lg
- 绪论
  - 人工智能
    - 研究领域
      - 感知
        - 计算机视觉
        - 语音信息处理
      - 认知
        - 知识表示
        - 推理
        - 规划
        - 决策
      - 机器学习
        - [有]监督学习
          - 回归 (标签连续)
          - 分类 (标签离散)
        - 无监督学习
          - 聚类
        - 强化学习
    - 流派
      - 符号主义(模拟人脑功能)
      - 连接主义(模拟人脑结构)
      - 融合派
  - 机器学习
    - [何为机器学习: little-sample](#little-sample)
    - 流程
      - 原始数据->数据预处理->特征提取->特征转换->预测->结果
    - 数据预处理
      - 完整性
      - 噪声
      - 是否匹配
      - 重复
      - 连续/离散
      - 样本分布是否平衡
    - 特征提取
      - 原始特征的线性组合获取新特征
    - 特征转换
      - 升维
      - 降维
        - PCA
          - 主成分分析
          - 二维数据类比分析
          - LDA充分利用数据的标签信息
            - 将数据按照标签根据同类数据间距离最小
            - 不同类数据间距离最大的原则映射
            - 一般LDA降维后可以直接分类
        - LDA
          - 线性判断分析
          - 可视为有监督的降维算法,实际无监督
          - PCA将数据整体映射到包含最多原始数据信息的低维空间中
            - 映射不包含任何数据内部的分类信息
            - 因此一般PCA降维后数据表示更有效但或许分类更加困难
    - 预测
  - 表示学习
    - 表示
      - 为了提高机器学习系统的准确率,就需要将输入信息转换为有效的特征
    - 数据表示是机器学习的核心问题
    - 底层特征与高层语义之间存在语义鸿沟,如何在鸿沟上搭桥是表示学习的关键
    - 表示方法
      - 局部表示/离散表示/符号表示
      - 分布式表示
        - 表示能力强
        - 向量维度低
        - 相似度容易计算
    - 嵌入
      - 将一个度量空间中的一些对象映射到另一个低维的度量空间中
      - 并尽可能保持不同对象之间的拓扑关系,比如自然语言中词的分布式表示
  - 深度学习
    - 一个好的表示学习策略必须具备一定的深度
    - 特征
      - 像是宰鱼要分多步,每步使用不同方式/工具
      - 通常从底层特征开始经过多步非线性转换才能得到好的高层语义表示
    - 特点
      - 增加特征重用性,指数级增加表示能力
    - [表示学习与深度学习关系](https://www.helloimg.com/images/2022/03/10/RCdJ0c.png)
    - 关键问题: 贡献度分配
      - 不同组件/参数对系统输出结果的影响权重
  - 神经网路
    - 人工神经网络
      - [激活函数](#激活函数)
      - [感知器](#感知器)
```

<a>![分割线](https://cdn.jsdelivr.net/gh/Weidows/Images/img/divider.png)</a>

## 引例

### little-sample

- 随机取市场上芒果样本

  `训练数据为芒果特征`: 颜色,大小,形状,产地,品牌

  `输出变量为芒果品质`: 甜蜜,多汁,成熟度

  ***

设计`学习算法`学习`训练数据`生成 `特征<->品质` 之间的相关性`模型`

下次买芒果时可以根据要购买芒果的特征,使用此模型预测芒果的品质

---

### 激活函数

![RaxGG1.png](https://www.helloimg.com/images/2022/03/18/RaxGG1.png)

- 上图为一个阶跃函数,其阈值是可以改变的,例:

  $$
  sgn(x) =
  \begin{cases}
    1 & if \ x > 0 \\
    0 & if \ x = 0 \\
    -1 & if \ x < 0
  \end{cases}
  $$

---

### 感知器

- 感知器可以认为是最简单的人工神经网络，只有一个神经元.

  ![](https://www.helloimg.com/images/2022/03/10/RC0tEP.png)

  $$
  y = f(w1 \cdot x1 + w2 \cdot x2 + b)
  $$

  当调整 $w,b$ 时,可以得到不同的变体逻辑运算;

  给定训练数据集, $w,b$ 可以通过学习自动调整

  ***

- 学习规则核心思想: `错误驱动`

  1. 权值初始化
  2. 输入样本对
  3. 计算输出
  4. 根据感知器学习规则调整权值
  5. 返回到步骤 2. 输入下一对样本,周而复始直到对所有样本,感知器的实际输出与期望输出相等

  ***

#### 例题

![](https://www.helloimg.com/images/2022/03/19/Raxa9S.png)

1. 初始化向量

   $$
   W(0) = (0.5,1,-1,0)^T \rArr W^T(0) = (0.5,1,-1,0)
   $$

2. 输入样本
3. 计算输出

   $$
   W^T(0)X^1 = (0.5,1,-1,0) \cdot (-1,1,-2,0)^T = 2.5
   $$

4. 调整权值,这里使用上面[#激活函数](#激活函数)的阶跃函数例子

   权向量第一个分量也就是 0.5 为阈值

   $$
   \begin{aligned}
    o^1(0) &= sgn(W^T(0)X^1)
    \\&= sgn(2.5) = 1

    \\ \ \\

    W(1) &=W(0) + \eta \left[d^{1}-o^{1}(0)\right] X^{1}
    \\&= (0.5,1,-1,0)^{T} + 0.1 (- 1- 1)(-1,1,-2,0)^{T}
    \\&= \left(\mathbf{0.7,0.8,-0.6,0)^{T}}\right.

    \\ \ \\

    W(2) &=W(1) + \eta \left[d^{2}-o^{2}(1)\right] X^{2}
    \\&= (0.7,0.8,-0.6,0)^{T} + 0.1 [- 1-(- 1)](-1,0,1.5,-0.5)^{T}
    \\&= \left(\mathbf{0.7,0.8,-0.6,0)^{T}}\right.

    \\ \ \\

    W(3) &=W(2) + \eta \left[d^{3}-o^{3}(2)\right] X^{3}
    \\&= (0.7,0.8,-0.6,0)^{T} + 0.1 [1-(- 1)](-1,-1,1,0.5)^{T}
    \\&= \left(\mathbf{0.5,0.6,-0.4,0.1)^{T}}\right.
   \end{aligned}
   $$

5. 返回 2. 直到

   $$
   d^{P} - o^{P} = 0 \qquad (p = 1,2,3)
   $$

<a>![分割线](https://cdn.jsdelivr.net/gh/Weidows/Images/img/divider.png)</a>

## 借物表

<a name='cite_note-1' href='#cite_ref-1'>[1]</a>: [一文看懂人工智能、机器学习、深度学习与神经网络之间的区别与关系](https://zhuanlan.zhihu.com/p/86794447)

<a name='cite_note-2' href='#cite_ref-2'>[2]</a>: [神经网络与深度学习](https://nndl.github.io/)
